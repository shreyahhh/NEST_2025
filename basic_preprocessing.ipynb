{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b9576f1-e682-4206-8ba7-8d1a253d98e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "For a sparse output, all columns should be a numeric or convertible to a numeric.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, Xs)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 ]\n\u001b[1;32m    834\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    836\u001b[0m                     \u001b[0;34m\"For a sparse output, all columns should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_hstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m         \"\"\"Stacks Xs horizontally.\n\u001b[0m\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    878\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"numpy.array_api\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Use NumPy API to support order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1996\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         if (\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Acute Myeloid Leukemia'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStudy Recruitment Rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Transform features using the preprocessor\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m X_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# **Step 5: Split the data**\u001b[39;00m\n\u001b[1;32m     71\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_encoded, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:751\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_output(Xs)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_output_indices(Xs)\n\u001b[0;32m--> 751\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mXs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:835\u001b[0m, in \u001b[0;36mColumnTransformer._hstack\u001b[0;34m(self, Xs)\u001b[0m\n\u001b[1;32m    830\u001b[0m         converted_Xs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    831\u001b[0m             check_array(X, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    832\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m Xs\n\u001b[1;32m    833\u001b[0m         ]\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 835\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    836\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor a sparse output, all columns should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    837\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe a numeric or convertible to a numeric.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    838\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39mhstack(converted_Xs)\u001b[38;5;241m.\u001b[39mtocsr()\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: For a sparse output, all columns should be a numeric or convertible to a numeric."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'processed_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# **Step 1: Parse the `Study Design` column**\n",
    "design_features = data['Study Design'].str.extract(\n",
    "    r'Allocation:\\s*(\\w+)\\|Intervention Model:\\s*([\\w\\s]+)\\|Masking:\\s*([\\w\\s\\(\\),]+)\\|Primary Purpose:\\s*(\\w+)',\n",
    "    expand=True\n",
    ")\n",
    "design_features.columns = ['Allocation', 'Intervention Model', 'Masking', 'Primary Purpose']\n",
    "\n",
    "# Add the parsed columns back to the dataset\n",
    "data = pd.concat([data.drop(columns=['Study Design']), design_features], axis=1)\n",
    "\n",
    "# Ensure unique column names\n",
    "data = data.loc[:, ~data.columns.duplicated()]\n",
    "\n",
    "# **Step 2: Identify columns**\n",
    "categorical_columns = [\n",
    "    'Study Status', 'Study Results', 'Sex', 'Age', 'Phases',\n",
    "    'Enrollment', 'Funder Type', 'Study Type', 'Sponsor',\n",
    "    'Allocation', 'Intervention Model', 'Masking', 'Primary Purpose'\n",
    "]\n",
    "numerical_columns = [col for col in data.columns if col not in categorical_columns + ['Study Recruitment Rate', 'NCT Number']]\n",
    "\n",
    "# **Step 3: Define preprocessing for categorical columns**\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "# Combine all preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_columns),\n",
    "        ('cat', categorical_preprocessor, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a TF-IDF vectorizer for the 'Conditions' column\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # Limit features to 1000 for efficiency\n",
    "\n",
    "# Update the ColumnTransformer to include TF-IDF for 'Conditions'\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', numerical_columns),  # Numerical columns remain as-is\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns),  # Encode categorical columns\n",
    "        ('text', tfidf_vectorizer, 'Conditions')  # TF-IDF vectorization for text\n",
    "    ]\n",
    ")\n",
    "\n",
    "# **Step 4: Prepare features (X) and target (y)**\n",
    "X = data.drop(columns=['Study Recruitment Rate', 'NCT Number'])\n",
    "y = data['Study Recruitment Rate']\n",
    "\n",
    "# Transform features using the preprocessor\n",
    "X_encoded = preprocessor.fit_transform(X)\n",
    "\n",
    "# **Step 5: Split the data**\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "smape_value = smape(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"RÂ² Score: {r2}\")\n",
    "print(f\"SMAPE: {smape_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6355087-0dba-44c3-83d6-e1df392ff10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of values in 'Phases':\n",
      "\n",
      "Phases\n",
      "PHASE1           9098\n",
      "PHASE2           5519\n",
      "PHASE3           3996\n",
      "PHASE1|PHASE2    1591\n",
      "PHASE2|PHASE3     382\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\"Phases\"]\n",
    "\n",
    "# Apply value_counts() to each of the specified columns and store the results\n",
    "for col in columns_to_check:\n",
    "    print(f\"Frequency of values in '{col}':\\n\")\n",
    "    print(data[col].value_counts())\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3d6bc1c-ce2b-44e6-989b-d142c0071e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of values in 'Conditions':\n",
      "\n",
      "Conditions\n",
      "Healthy                                                            1337\n",
      "Healthy Volunteers                                                  611\n",
      "COVID-19                                                            261\n",
      "Healthy Participants                                                192\n",
      "Healthy Subjects                                                    178\n",
      "                                                                   ... \n",
      "Bipolar Depression|Suicidal Ideation and Behavior                     1\n",
      "Infected Atopic Dermatitis/Eczema                                     1\n",
      "Endometrial Cancer|Uterine Cancer|Ovarian Cancer|Carcinosarcoma       1\n",
      "Thrombocytopenia|Hematologic Diseases|Bone Marrow Aplasia             1\n",
      "Thrombus                                                              1\n",
      "Name: count, Length: 7837, dtype: int64\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns_to_check = [\"Conditions\"]\n",
    "\n",
    "# Apply value_counts() to each of the specified columns and store the results\n",
    "for col in columns_to_check:\n",
    "    print(f\"Frequency of values in '{col}':\\n\")\n",
    "    print(data[col].value_counts())\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e6349-d769-4042-9877-653ca999a9fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
